name: docrag
mode: docs
start_loc: /docs/
exclude:
  - "*.html"

# Optional: Web crawling specific parameters
doc_params:
  recursive: true               # Process subdirectories recursively (default: true)
  include_hidden: false         # Include hidden files (default: false)
  follow_symlinks: false        # Follow symbolic links (default: false)
  max_file_size_mb: 100         # Maximum file size in MB (optional)
  file_extensions:              # Only process these file types (optional)
    - .pdf
    - .docx
    - .txt
    - .md

# Optional: General parameters for downstream RAG processing
rag_params:
  embedding_model: sentence-transformers/all-mpnet-base-v2
  embedding_dim: 768              # Embedding dimension (default: 384)
  chunk_size: 512                # Chunk size for text splitting (default: 1024)
  chunk_overlap: 50
  distance_metric: angular        # Distance metric: angular, euclidean, etc. (default: angular)
  max_tokens: 8192

# Optional: LLM configuration for chat/RAG
# Option 1: OpenRouter (default if not specified, requires OPENROUTER_API_KEY env var)
# llm_params:
#   provider: openrouter
#   model: anthropic/claude-3.5-sonnet  # Optional, can also use OPENROUTER_MODEL env var
#   base_url: https://openrouter.ai/api/v1  # Optional, defaults to OpenRouter
#   api_key: your-api-key-here     # Optional, can also use OPENROUTER_API_KEY env var

# Option 2: Ollama (local LLM)
# llm_params:
#   provider: ollama
#   model: llama3.2:3b             # Or any other Ollama model you have installed
#   base_url: http://localhost:11434/v1  # Default Ollama API endpoint

# Option 3: OpenAI-compatible endpoint (e.g., llama.cpp, vLLM, LocalAI, etc.)
# llm_params:
#   provider: openai-compatible
#   model: your-model-name
#   base_url: http://localhost:8080/v1
#   api_key: optional-key          # Some endpoints require an API key
