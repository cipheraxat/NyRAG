name: webrag
mode: web
start_loc: https://vespa.ai/
exclude:
  - https://vespa.ai/pricing
  - https://vespa.ai/sales
  - https://status.vespa.ai/*

# Optional: Web crawling specific parameters
crawl_params:
  respect_robots_txt: true     # Respect robots.txt rules (default: true)
  aggressive_crawl: false       # Enable aggressive crawling (higher speed, more requests) (default: false)
  follow_subdomains: true       # Follow subdomains of the start URL (default: true)
  strict_mode: false            # Only crawl URLs matching the start URL pattern (default: false)
  user_agent_type: chrome       # User agent type: chrome, firefox, safari, mobile, bot (default: chrome)
  # custom_user_agent: "..."    # Custom user agent string (optional, overrides user_agent_type)
  allowed_domains:            # Explicitly allowed domains (optional, auto-detected from start_loc)
    - vespa.ai
    - docs.vespa.ai

# Optional: General parameters for downstream RAG processing
rag_params:
  embedding_model: sentence-transformers/all-MiniLM-L6-v2
  embedding_dim: 384              # Embedding dimension (default: 384)
  chunk_size: 1024                # Chunk size for text splitting (default: 1024)
  chunk_overlap: 50
  distance_metric: angular        # Distance metric: angular, euclidean, etc. (default: angular)
  max_tokens: 8192

# Optional: LLM configuration for chat/RAG
# Option 1: OpenRouter (default if not specified, requires OPENROUTER_API_KEY env var)
# llm_params:
#   provider: openrouter
#   model: anthropic/claude-3.5-sonnet  # Optional, can also use OPENROUTER_MODEL env var
#   base_url: https://openrouter.ai/api/v1  # Optional, defaults to OpenRouter
#   api_key: your-api-key-here     # Optional, can also use OPENROUTER_API_KEY env var

# Option 2: Ollama (local LLM)
# llm_params:
#   provider: ollama
#   model: llama3.2:3b             # Or any other Ollama model you have installed
#   base_url: http://localhost:11434/v1  # Default Ollama API endpoint

# Option 3: OpenAI-compatible endpoint (e.g., llama.cpp, vLLM, LocalAI, etc.)
# llm_params:
#   provider: openai-compatible
#   model: your-model-name
#   base_url: http://localhost:8080/v1
#   api_key: optional-key          # Some endpoints require an API key
